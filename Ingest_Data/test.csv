from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .master("local") \
    .appName("pokemon_data") \
    .getOrCreate()

sc = spark.sparkContext
sc.setLogLevel("WARN")

pokemon_df = sc.parallelize( \
    [(1,"Bulbasaur","Grass","Poison",318,45,49,49,65,65,45,1,False), \
    (2,"Ivysaur","Grass","Poison",405,60,62,63,80,80,60,1,False), \
    (3,"Venusaur","Grass","Poison",525,80,82,83,100,100,80,1,False), \
    (3,"VenusaurMega Venusaur","Grass","Poison",625,80,100,123,122,120,80,1,False), \
    (4,"Charmander","Fire", None,309,39,52,43,60,50,65,1,False), \
    (5,"Charmeleon","Fire", None,405,58,64,58,80,65,80,1,False), \
    (6,"Charizard","Fire","Flying",534,78,84,78,109,85,100,1,False), \
    (6,"CharizardMega Charizard X","Fire","Dragon",634,78,130,111,130,85,100,1,False), \
    (6,"CharizardMega Charizard Y","Fire","Flying",634,78,104,78,159,115,100,1,False)] \
).toDF(["ID","Name","Type 1","Type 2","Total","HP","Attack","Defense","Sp. Atk","Sp. Def","Speed","Generation","Legendary"])

pokemon_df.show(10)

pokemon_df = pokemon_df.dropDuplicates(["ID"])

pokemon_names = pokemon_df.select(["ID", "Name"])
pokemon_info = pokemon_df.select(["ID", "Generation", "Legendary"])

pokemon_names.createOrReplaceTempView("pokemon_names")
pokemon_info.createOrReplaceTempView("pokemon_info")

final_df = spark.sql("SELECT pokemon_names.ID, Name, Generation, Legendary FROM pokemon_names JOIN pokemon_info ON pokemon_names.ID=pokemon_info.ID ORDER BY pokemon_names.ID")

final_df.show(10)

#final_df.repartition(1).write.csv("file:/mnt/c/Users/GabrielKlein/Desktop/Big_Data_220711/Week_6/output")

spark.stop()